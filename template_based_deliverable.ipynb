{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "\n",
    "#all_features = [col for col in data.columns if col not in ['Track ID','TrackID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "features = [\n",
    "    'zero_cross_rate_mean','zero_cross_rate_std','rmse_mean','rmse_var',\n",
    "    'spectral_centroid_mean','spectral_centroid_var','spectral_bandwidth_mean','spectral_bandwidth_var',\n",
    "    'spectral_rolloff_mean','spectral_rolloff_var','spectral_contrast_mean','spectral_contrast_var',\n",
    "    'spectral_flatness_mean','spectral_flatness_var',\n",
    "    'chroma_stft_7_mean','tempo',\n",
    "    'mfcc_1_mean','mfcc_2_mean','mfcc_3_mean','mfcc_4_mean','mfcc_5_mean','mfcc_6_mean',\n",
    "    'mfcc_2_std','mfcc_3_std','mfcc_4_std','mfcc_5_std', 'mfcc_7_std'\n",
    "]\n",
    "\n",
    "targets = ['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, mean, covariance, datapoints, accumulated_distance=0.0, inv_covariance=None):\n",
    "        self.mean = mean\n",
    "        self.covariance = covariance\n",
    "        self.datapoints = datapoints\n",
    "        self.accumulated_distance = accumulated_distance\n",
    "        self.inv_covariance = inv_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "\n",
    "def euclidean_distance(datapoint, mu):\n",
    "    '''\n",
    "    Compute the Euclidean distance between two vectors.\n",
    "    '''\n",
    "    distance = 0\n",
    "    for i in range(len(datapoint)):\n",
    "        distance += (datapoint[i] - mu[i]) ** 2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "def calculate_mean(X):\n",
    "    '''\n",
    "    Compute the mean vector of the given data.\n",
    "    '''\n",
    "    n_samples, n_features = X.shape\n",
    "    accumalated_sum = np.zeros(n_features)\n",
    "    for k in range(n_samples):\n",
    "        accumalated_sum += X[k][:]\n",
    "    \n",
    "    mean = accumalated_sum/n_samples\n",
    "    return mean\n",
    "\n",
    "def calculate_covariance(X, mean):\n",
    "    '''\n",
    "    Compute the covariance matrix for the given data.\n",
    "    '''\n",
    "    n_samples, n_features = X.shape\n",
    "    covariance_matrix = np.zeros((n_features, n_features))\n",
    "    for k in range(n_samples):\n",
    "        diff = X[k] - mean\n",
    "        covariance_matrix += np.outer(diff, diff)\n",
    "\n",
    "    covariance_matrix /= n_samples\n",
    "    return covariance_matrix\n",
    "\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    '''\n",
    "    Calculate the mahalanobis distance between\n",
    "    '''\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        cov += np.eye(cov.shape[0]) * 1e-6\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "    diff = x - mean\n",
    "    return diff.T @ inv_cov @ diff\n",
    "\n",
    "\n",
    "def calculate_accumulated_distance(X, mean, covariance):\n",
    "    '''\n",
    "    Calculate the sum of Mahalanobis distances of all points in X to the mean.\n",
    "    '''\n",
    "    n_samples, _ = X.shape\n",
    "    accumulated_distance = 0.0\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        distance = mahalanobis_distance(X[k], mean, covariance)\n",
    "        accumulated_distance += distance\n",
    "\n",
    "    return accumulated_distance  # total D\n",
    "\n",
    "\n",
    "def find_cluster_to_split(cluster_dict, current_class):\n",
    "    '''\n",
    "    Find the cluster with the largest accumulated distance for splitting.\n",
    "    '''\n",
    "    clusters = cluster_dict[current_class]\n",
    "    if(len(clusters) == 1):\n",
    "        return clusters[0]\n",
    "    else:\n",
    "        \n",
    "        cluster_to_split = clusters[0]\n",
    "        for i in range(1, len(clusters)):\n",
    "            if(clusters[i].accumulated_distance > cluster_to_split.accumulated_distance):\n",
    "                cluster_to_split = clusters[i] \n",
    "\n",
    "    return cluster_to_split\n",
    "\n",
    "def find_cluster_to_split_points(cluster_dict, current_class):\n",
    "    \"\"\"\n",
    "    Return the cluster in `cluster_dict[current_class]` that has\n",
    "    the largest number of datapoints.\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters = cluster_dict[current_class]\n",
    "    if len(clusters) == 1:\n",
    "        return clusters[0]\n",
    "    \n",
    "    return max(clusters, key=lambda c: len(c.datapoints))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def scale_data(X_train, X_test):\n",
    "    '''\n",
    "    Compute the mean vector of the given data.\n",
    "    '''\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "\n",
    "    means = np.mean(X_train_np, axis=0)\n",
    "    stds = np.std(X_train_np, axis=0)\n",
    "    \n",
    "    X_train_scaled = (X_train_np - means) / stds\n",
    "    X_test_scaled = (X_test_np - means) / stds\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def calculate_accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(X_train, y_train, classes, beta):\n",
    "    \"\"\"\n",
    "    Creates a template-based classifier by clustering data for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.DataFrame or numpy.ndarray\n",
    "        Training feature data.\n",
    "    y_train : pandas.Series or numpy.ndarray\n",
    "        Training labels.\n",
    "    classes : list\n",
    "        List of unique class labels.\n",
    "    beta : float\n",
    "        Stopping criterion for cluster splitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cluster_dict : dict\n",
    "        Dictionary mapping class labels to lists of Cluster objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary mapping class labels to lists of Cluster objects.\n",
    "    cluster_dict = {}\n",
    "    for current_class in classes:\n",
    "        cluster_dict[current_class] = []\n",
    "        clusters = cluster_dict[current_class]\n",
    "\n",
    "        class_data = X_train[y_train['Genre'] == current_class].to_numpy()\n",
    "\n",
    "        # Initial cluster values\n",
    "        mean = calculate_mean(class_data)\n",
    "        cov = calculate_covariance(class_data, mean)\n",
    "        acc_dist = calculate_accumulated_distance(class_data, mean, cov)\n",
    "        \n",
    "        cluster_main = Cluster(mean=mean, covariance=cov, datapoints=class_data, accumulated_distance=acc_dist)\n",
    "\n",
    "        clusters.append(cluster_main)\n",
    "        we_are_improving = True\n",
    "\n",
    "        previous_accumulated_distance = 0\n",
    "        for cluster in clusters:\n",
    "                previous_accumulated_distance += cluster.accumulated_distance\n",
    "        \n",
    "        # Removing this entire while loop increases accuracy...\n",
    "        while we_are_improving: \n",
    "            current_accumulated_distance = 0\n",
    "\n",
    "            splitting_cluster = find_cluster_to_split_points(cluster_dict, current_class)\n",
    "            mu = splitting_cluster.mean\n",
    "            cov = splitting_cluster.covariance\n",
    "            \n",
    "            # Use the variance of the splitting cluster to generate a noise vector\n",
    "            stds = np.sqrt(np.diag(cov))\n",
    "            scale_factor = 1.0\n",
    "            w = scale_factor * np.random.uniform(-stds, stds)\n",
    "\n",
    "            # Altered mean for the new two clusters\n",
    "            mu_1 = mu + w\n",
    "            mu_2 = mu - w\n",
    "\n",
    "            trial_clusters = [c for c in clusters if c is not splitting_cluster]\n",
    "            trial_clusters.append(Cluster(mean=mu_1, covariance=cov, datapoints=[]))\n",
    "            trial_clusters.append(Cluster(mean=mu_2, covariance=cov, datapoints=[]))\n",
    "\n",
    "            # Assign all datapoints to the closest of the trial clusters\n",
    "            for cluster in trial_clusters:\n",
    "                cluster.datapoints = []\n",
    "            for x in class_data:\n",
    "                closest_cluster = trial_clusters[0]\n",
    "                closest_distance = euclidean_distance(x, closest_cluster.mean)\n",
    "                for c in range(1, len(trial_clusters)):\n",
    "                    if euclidean_distance(x, trial_clusters[c].mean) < closest_distance:\n",
    "                        closest_distance = euclidean_distance(x, trial_clusters[c].mean)\n",
    "                        closest_cluster = trial_clusters[c]\n",
    "                closest_cluster.datapoints.append(x)\n",
    "        \n",
    "            \n",
    "            # Recalculate means/covs/accumulated_distance for trial clusters\n",
    "            current_accumulated_distance = 0\n",
    "            for cluster in trial_clusters:\n",
    "                if len(cluster.datapoints) == 0:\n",
    "                    continue\n",
    "                X_cluster = np.array(cluster.datapoints)\n",
    "                cluster.mean = calculate_mean(X_cluster)\n",
    "                cluster.covariance = calculate_covariance(X_cluster, cluster.mean)\n",
    "                cluster.accumulated_distance = calculate_accumulated_distance(X_cluster, cluster.mean, cluster.covariance)\n",
    "                current_accumulated_distance += cluster.accumulated_distance\n",
    "\n",
    "            # Only commit the split if it improved\n",
    "            if current_accumulated_distance < beta * previous_accumulated_distance:\n",
    "                clusters.clear()\n",
    "                clusters.extend(trial_clusters)\n",
    "                previous_accumulated_distance = current_accumulated_distance\n",
    "            else:\n",
    "                we_are_improving = False\n",
    "\n",
    "\n",
    "\n",
    "    return cluster_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_unscaled, y_train = train[features], train[targets]\n",
    "X_test_unscaled, y_test = test[features], test[targets]\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train_unscaled,X_test_unscaled)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train_unscaled.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test_unscaled.columns)\n",
    "\n",
    "classes = y_train['Genre'].unique()\n",
    "#print(f\"Classes: {classes}\")\n",
    "beta = 0.99\n",
    "cluster_dict = create_classifier(X_train, y_train, classes, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in classes:\n",
    "    print(f\"{i} has {len(cluster_dict[i])} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_for_class(class_name, cluster_dict, X_train, y_train):\n",
    "    class_data = X_train[y_train['Genre'] == class_name]\n",
    "\n",
    "    #Reduces to 2 dimensions for plotting\n",
    "    pca = PCA(n_components=2) \n",
    "    class_data_2d = pca.fit_transform(class_data)\n",
    "\n",
    "    # Transform cluster-means\n",
    "    cluster_means_2d = [pca.transform(cluster.mean.reshape(1, -1))[0] for cluster in cluster_dict[class_name]]\n",
    "\n",
    "    # Plot datapunktene\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(class_data_2d[:, 0], class_data_2d[:, 1], c='lightgray', label='Data points')\n",
    "\n",
    "    # Plot cluster sentrene\n",
    "    for idx, mean_2d in enumerate(cluster_means_2d):\n",
    "        plt.scatter(mean_2d[0], mean_2d[1], c='red', marker='X', s=100, label=f'Cluster' if idx == 0 else \"\")\n",
    "    \n",
    "    plt.title(f\"Clusters for class '{class_name}'\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_for_class('blues', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('classical', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('country', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('disco', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('hiphop', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('jazz', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('metal', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('pop', cluster_dict, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sample(x,cluster_dict):\n",
    "    best_distance = float('inf')\n",
    "    predicted_class = ''\n",
    "\n",
    "    for class_name, clusters in cluster_dict.items(): # For key, values in dict\n",
    "        for cluster in clusters:\n",
    "            d = mahalanobis_distance(x,cluster.mean, cluster.covariance)\n",
    "            if d < best_distance:\n",
    "                best_distance = d\n",
    "                predicted_class = class_name\n",
    "    return predicted_class\n",
    "    \n",
    "\n",
    "predicted_label = classify_sample(X_test.iloc[0],cluster_dict)\n",
    "print(f\"Predicted label: {predicted_label}. True label: {y_test.iloc[0]['Genre']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, cluster_dict):\n",
    "    predictions = []\n",
    "    for x in X_test.to_numpy():\n",
    "        label = classify_sample(x, cluster_dict)\n",
    "        predictions.append(label)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_test_true = y_test['Genre'].to_numpy()\n",
    "y_pred = predict(X_test, cluster_dict)\n",
    "\n",
    "accuracy = calculate_accuracy(y_test_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
