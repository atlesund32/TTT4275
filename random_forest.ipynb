{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Training:\n",
    "- Get a random subset of the dataset\n",
    "- Create a decision tree with this random subset.\n",
    "- Repeat for as many times as the number of trees that we want.\n",
    "\n",
    "Testing (given a datapoint we want to test)\n",
    "- Get the predictions from each tree\n",
    "- Hold a majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(y):\n",
    "    '''\n",
    "    Find the most common label in a list of labels.\n",
    "    '''\n",
    "    counts = {}\n",
    "    for label in y:                 \n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    return max(counts, key=counts.get)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None): #value will only be passed to leaf nodes\n",
    "        '''\n",
    "        Initialize a node in the decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature : int\n",
    "            Index of the feature used for splitting\n",
    "        threshold : float\n",
    "            Threshold value for splitting\n",
    "        left : Node\n",
    "            Left child node\n",
    "        right : Node\n",
    "            Right child node\n",
    "        value : int\n",
    "            Value for leaf nodes (class label)\n",
    "        '''\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold # The split value for a decision node\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, min_samples_split=3, max_depth=10, n_features=None):\n",
    "        '''\n",
    "        Initialize a decision tree classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_samples_split : int \n",
    "            Minimum number of samples required to split an internal node\n",
    "        max_depth : int \n",
    "            Maximum depth of the tree\n",
    "        n_features : int \n",
    "            Number of features considered when looking for the best split\n",
    "        '''\n",
    "       \n",
    "        self.min_samples_split = min_samples_split \n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features \n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Build a decision tree classifier from the training set (X, y).\n",
    "        '''\n",
    "        if not self.n_features:\n",
    "            self.n_features = X.shape[1]\n",
    "        else:\n",
    "            self.n_features = min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X,y)\n",
    "\n",
    "    # Recursive function to grow the tree\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        '''\n",
    "        Recursively grow the decision tree.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Node\n",
    "            Root node of the grown tree\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            n_labels == 1):\n",
    "            leaf_value = most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        subset_features = np.random.choice(n_features, self.n_features, replace=False) # Only use a subset of unique features\n",
    "\n",
    "        # Find the best split\n",
    "        best_thresh, best_feature = self._best_split(X, y, subset_features)\n",
    "        \n",
    "        # Create the left and right subtrees\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:,best_feature],best_thresh)\n",
    "\n",
    "        if len(left_idxs)  == 0 or len(right_idxs) == 0:\n",
    "            leaf_value = most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    # Finding the best splits and thresholds \n",
    "    def _best_split(self, X, y, subset_features):\n",
    "        '''\n",
    "        Find the best feature and threshold to split on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (best_threshold, best_feature_index)\n",
    "        '''\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feature in subset_features:\n",
    "            X_column = X[:, feature]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature\n",
    "                    split_thresh = threshold\n",
    "        return split_thresh, split_idx\n",
    "\n",
    "\n",
    "    def _information_gain(self,y,X_column, threshold):\n",
    "        '''\n",
    "        Calculate the information gain of a split:\n",
    "        E(parent) - [weighted average] * E(children)\n",
    "        '''\n",
    "\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if(len(left_idxs) == 0 or len(right_idxs) == 0):\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "    \n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        '''\n",
    "        Split the data based on a threshold value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (left_indices, right_indices)\n",
    "        '''\n",
    "        left_idxs = np.argwhere(X_column<=split_thresh).flatten() \n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    def _entropy(self,y):\n",
    "        '''\n",
    "        Calculate the entropy of a set of labels:\n",
    "        - Sum(p(X)*log_2(p(X)))\n",
    "        '''\n",
    "        # Creates a historgram [instances_of_0, instances_of_1, ...]\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist/len(y)\n",
    "        \n",
    "        # Only keep probabilities > 0\n",
    "        ps_nz = ps[ps > 0]\n",
    "        return -np.sum(ps_nz * np.log(ps_nz)) \n",
    "\n",
    "    \n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        '''\n",
    "        Traverse the tree to make a prediction for a single sample.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like of shape of features\n",
    "            Single sample to predict\n",
    "        node : Node\n",
    "            Current node in the tree. The process is recursive\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int or str\n",
    "            Predicted class label\n",
    "\n",
    "        '''\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x,node.left)\n",
    "        return self._traverse_tree(x,node.right)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict class labels for samples in X.\n",
    "        '''\n",
    "        return np.array([self._traverse_tree(x,self.root) for x in X])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_trees=200, max_depth=10, min_samples_split=3, n_features=None):\n",
    "        '''\n",
    "        Initialize a random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trees : int, optional\n",
    "            Number of trees in the forest\n",
    "        max_depth : int, optional\n",
    "            Maximum depth for each tree\n",
    "        min_samples_split : int, optional\n",
    "            Minimum number of samples required to split an internal node\n",
    "        n_features : int, optional\n",
    "            Number of features to consider when looking for the best split\n",
    "        '''\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "\n",
    "    def fit(self, X,y):\n",
    "        '''\n",
    "        Build a random forest classifier from the training set (X, y).\n",
    "        '''\n",
    "\n",
    "        n_tot_features = X.shape[1]\n",
    "        if self.n_features is None:\n",
    "            self.n_features = int(np.sqrt(n_tot_features))\n",
    "        else:\n",
    "            self.n_features = min(self.n_features, n_tot_features) \n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth,\n",
    "                        min_samples_split=self.min_samples_split,\n",
    "                         n_features=self.n_features)\n",
    "            X_sample, y_sample =self._bootstrap_samples(X,y)\n",
    "            tree.fit(X_sample,y_sample)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def _bootstrap_samples(self,X,y):\n",
    "        '''\n",
    "        Create bootstrap samples from the training data.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (X_bootstrap, y_bootstrap)\n",
    "        '''\n",
    "        n_samples, _ = X.shape\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True) # sampling with replacement\n",
    "        return X[idxs], y[idxs]\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees]) # [[pred_1st_sample_by_1st_tree, pred_2st_sample_by_1st_tree],\n",
    "                                                                        #   [pred_1st_sample_by_2nd_tree],[pred_2nd_sample_by_2nd_tree],\n",
    "                                                                        #   [...], ...]\n",
    "        \n",
    "        #[[all predictions of first sample], [all predictions of second sample], [...], ...]\n",
    "        tree_predictions = np.swapaxes(predictions, 0, 1)\n",
    "        predictions = np.array([most_common_label(pred) for pred in tree_predictions])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df1 = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "# df2 = pd.read_csv('data/GenreClassData_10s.txt', sep='\\t')\n",
    "# df3 = pd.read_csv('data/GenreClassData_5s.txt', sep='\\t')\n",
    "\n",
    "# frames = [df1, df2, df3]\n",
    "# result = pd.concat(frames)\n",
    "# print(f\"{len(result)} rows in the result, {len(df1)} in df1, {len(df2)} in df2, {len(df3)} in df3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43438696, 0.27287016, 0.70602324, ..., 0.25229837, 0.13490084,\n",
       "        0.1865418 ],\n",
       "       [0.30832523, 0.2935099 , 0.73328814, ..., 0.41676686, 0.21867656,\n",
       "        0.38473969],\n",
       "       [0.29358315, 0.3303101 , 0.40756804, ..., 0.38169812, 0.2802361 ,\n",
       "        0.31613867],\n",
       "       ...,\n",
       "       [0.42687408, 0.22619052, 0.39196788, ..., 0.11390357, 0.06748416,\n",
       "        0.10959762],\n",
       "       [0.09678806, 0.10171028, 0.10111576, ..., 0.21896671, 0.15575828,\n",
       "        0.38902993],\n",
       "       [0.2628941 , 0.1523535 , 0.51456561, ..., 0.35766246, 0.27377917,\n",
       "        0.57002067]], shape=(792, 63))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "result = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "\n",
    "features = [col for col in result.columns if col not in ['Track ID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "targets = ['GenreID'] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = result[result['Type'] == 'Train'].copy()\n",
    "test = result[result['Type'] == 'Test'].copy()\n",
    "# feature data\n",
    "X_train, y_train = train[features], train[targets]\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.to_numpy().ravel()  # flater ut fra (n,1) til (n,)\n",
    "y_test  = y_test.to_numpy().ravel()\n",
    "\n",
    "X_test.view()\n",
    "X_train.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x1e49f8387c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForest()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "acc = accuracy(y_test, predictions)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per‐class performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7083    0.8500    0.7727        20\n",
      "           1     0.8000    1.0000    0.8889        20\n",
      "           2     0.4211    0.4000    0.4103        20\n",
      "           3     0.7619    0.8000    0.7805        20\n",
      "           4     0.7727    0.8500    0.8095        20\n",
      "           5     0.9412    0.8421    0.8889        19\n",
      "           6     0.5000    0.3000    0.3750        20\n",
      "           7     0.7727    0.8500    0.8095        20\n",
      "           8     0.6667    0.6316    0.6486        19\n",
      "           9     0.8333    0.7500    0.7895        20\n",
      "\n",
      "    accuracy                         0.7273       198\n",
      "   macro avg     0.7178    0.7274    0.7173       198\n",
      "weighted avg     0.7169    0.7273    0.7168       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classes = np.unique(train['GenreID'])\n",
    "print(\"Per‐class performance:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    predictions, \n",
    "    labels=classes,       \n",
    "    digits=4,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
