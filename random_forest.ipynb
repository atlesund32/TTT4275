{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Training:\n",
    "- Get a random subset of the dataset\n",
    "- Create a decision tree with this random subset.\n",
    "- Repeat for as many times as the number of trees that we want.\n",
    "\n",
    "Testing (given a datapoint we want to test)\n",
    "- Get the predictions from each tree\n",
    "- Hold a majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*, value=None): #value will only be passed to leaf nodes\n",
    "        # Parameters after ,*, are keyword-arguments only, has to be addressed as value=42, and not (...,42)\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold # The split value for the feature\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=10, n_features=None):\n",
    "        self.min_samples_split = min_samples_split # Minimum number of samples required to split an internal node, prevents overfitting\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features \n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X,y)\n",
    "\n",
    "    # Recursive function to grow the tree\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Check if we have reached the stopping criteria\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            n_labels == 1):\n",
    "\n",
    "\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        subset_features = np.random.choice(n_features, self.n_features, replace=False) # Only use a subset of unique features\n",
    "\n",
    "        # Find the best split\n",
    "        best_thresh, best_feature = self._best_split(X, y, subset_features)\n",
    "        \n",
    "        # Create the left and right subtrees\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:,best_feature],best_thresh)\n",
    "\n",
    "        if len(left_idxs)  == 0 or len(right_idxs) == 0:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    # Finding the best splits and thresholds \n",
    "    def _best_split(self, X, y, subset_features):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feature in subset_features:\n",
    "            X_column = X[:, feature]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature\n",
    "                    split_thresh = threshold\n",
    "        return split_thresh, split_idx\n",
    "\n",
    "\n",
    "    def _information_gain(self,y,X_column, threshold):\n",
    "        # IG = E(parent) - [weighted average] * E(children)\n",
    "\n",
    "        #parent entropy\n",
    "\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        #create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if(len(left_idxs) == 0 or len(right_idxs) == 0):\n",
    "            return 0\n",
    "\n",
    "        #calculate the avg. weighted entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
    "\n",
    "        #calculate the IG\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column<=split_thresh).flatten() # gives the index of the arguments that fulfills the args. flatten() flattens the list of lists into a list\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    def _entropy(self,y):\n",
    "        # E = - Sum(p(X)*log_2(p(X)))\n",
    "        hist = np.bincount(y) #creates a historgram [instances_of_0, instances_of_1, ...]\n",
    "        ps = hist/len(y)\n",
    "        return -np.sum([p*np.log(p) for p in ps if p>0]) \n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # Return the most common label in y\n",
    "        counter = Counter(y) # REPLACE\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x,node.left)\n",
    "        return self._traverse_tree(x,node.right)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x,self.root) for x in X])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "# BaseEstimator, ClassifierMixin\n",
    "class RandomForest():\n",
    "    def __init__(self, n_trees=200, max_depth=10, min_samples_split=3, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "\n",
    "    def fit(self, X,y):\n",
    "\n",
    "        n_tot_features = X.shape[1]\n",
    "        if self.n_features is None:\n",
    "            self.n_features = int(np.sqrt(n_tot_features))\n",
    "        else:\n",
    "            self.n_features = min(self.n_features, n_tot_features) \n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth,\n",
    "                        min_samples_split=self.min_samples_split,\n",
    "                         n_features=self.n_features)\n",
    "            X_sample, y_sample =self._bootstrap_samples(X,y)\n",
    "            tree.fit(X_sample,y_sample)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def _bootstrap_samples(self,X,y):\n",
    "        n_samples, _ = X.shape\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True) # sampling with replacement\n",
    "        return X[idxs], y[idxs]\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        # Return the most common label in y\n",
    "        counter = Counter(y) # REPLACE\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees]) # [[pred_1st_sample_by_1st_tree, pred_2st_sample_by_1st_tree],\n",
    "                                                                        #   [pred_1st_sample_by_2nd_tree],[pred_2nd_sample_by_2nd_tree],\n",
    "                                                                        #   [...], ...]\n",
    "        #[[all predictions of first sample], [all predictions of second sample], [...], ...]\n",
    "\n",
    "        tree_predictions = np.swapaxes(predictions, 0, 1)\n",
    "        predictions = np.array([self._most_common_label(pred) for pred in tree_predictions])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "\n",
    "#all_features = [col for col in data.columns if col not in ['Track ID','TrackID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "all_features = [\n",
    "    'zero_cross_rate_mean','zero_cross_rate_std','rmse_mean','rmse_var',\n",
    "    'spectral_centroid_mean','spectral_centroid_var','spectral_bandwidth_mean','spectral_bandwidth_var',\n",
    "    'spectral_rolloff_mean','spectral_rolloff_var','spectral_contrast_mean','spectral_contrast_var',\n",
    "    'spectral_flatness_mean','spectral_flatness_var',\n",
    "    'chroma_stft_1_mean','chroma_stft_2_mean','chroma_stft_3_mean','chroma_stft_4_mean',\n",
    "    'chroma_stft_5_mean','chroma_stft_6_mean','chroma_stft_7_mean','chroma_stft_8_mean',\n",
    "    'chroma_stft_9_mean','chroma_stft_10_mean','chroma_stft_11_mean','chroma_stft_12_mean',\n",
    "    'chroma_stft_1_std','chroma_stft_2_std','chroma_stft_3_std','chroma_stft_4_std',\n",
    "    'chroma_stft_5_std','chroma_stft_6_std','chroma_stft_7_std','chroma_stft_8_std',\n",
    "    'chroma_stft_9_std','chroma_stft_10_std','chroma_stft_11_std','chroma_stft_12_std',\n",
    "    'tempo',\n",
    "    'mfcc_1_mean','mfcc_2_mean','mfcc_3_mean','mfcc_4_mean','mfcc_5_mean','mfcc_6_mean',\n",
    "    'mfcc_7_mean','mfcc_8_mean','mfcc_9_mean','mfcc_10_mean','mfcc_11_mean','mfcc_12_mean',\n",
    "    'mfcc_1_std','mfcc_2_std','mfcc_3_std','mfcc_4_std','mfcc_5_std','mfcc_6_std',\n",
    "    'mfcc_7_std','mfcc_8_std','mfcc_9_std','mfcc_10_std','mfcc_11_std','mfcc_12_std'\n",
    "]\n",
    "'''all_features = [\n",
    "    'zero_cross_rate_mean','zero_cross_rate_std','rmse_mean','rmse_var',\n",
    "    'spectral_centroid_mean','spectral_centroid_var','spectral_bandwidth_mean','spectral_bandwidth_var',\n",
    "    'spectral_rolloff_mean','spectral_rolloff_var','spectral_contrast_mean','spectral_contrast_var',\n",
    "    'spectral_flatness_mean','spectral_flatness_var',\n",
    "    'chroma_stft_7_mean',\n",
    "    \n",
    "    'tempo',\n",
    "    'mfcc_1_mean','mfcc_2_mean','mfcc_3_mean','mfcc_4_mean','mfcc_5_mean','mfcc_6_mean',\n",
    "\n",
    "    'mfcc_2_std','mfcc_3_std','mfcc_4_std','mfcc_5_std', 'mfcc_7_std'\n",
    "]'''\n",
    "#Erfaringer: \n",
    "#'chroma_stft_x_std' er elendig, drar ned accuracy\n",
    "features = all_features\n",
    "\n",
    "#features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'chroma_stft_10_mean']\n",
    "\n",
    "targets = ['GenreID'] \n",
    "\n",
    "# feature data\n",
    "X_train, y_train = train[features], train[targets]\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n",
    "X_train_np = X_train.to_numpy()      \n",
    "y_train_np = y_train.to_numpy().ravel()  # flater ut fra (n,1) til (n,)\n",
    "\n",
    "X_test_np  = X_test.to_numpy()\n",
    "y_test_np  = y_test.to_numpy().ravel()\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForest()\n",
    "clf.fit(X_train_np, y_train_np)\n",
    "\n",
    "#Beste param: {'max_depth': 10, 'min_samples_split': 2, 'n_trees': 150}\n",
    "#param: {'max_depth': 10, 'min_samples_split': 2, 'n_trees': 200} gave 0.712121\n",
    "# def __init__(self, n_trees=200, max_depth=10, min_samples_split=3, n_features=None): gave 0.727272\n",
    "#def __init__(self, n_trees=200, max_depth=10, min_samples_split=4, n_features=None): gave 0.712121\n",
    "\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_trees': [150, 200],\n",
    "#     'max_depth': [10, 15],\n",
    "#     'min_samples_split': [2, 4],\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(\n",
    "#     estimator=RandomForest(),\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid.fit(X_train_np, y_train_np)\n",
    "# print(\"Beste param:\", grid.best_params_)\n",
    "# print(\"Best score:\", grid.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "predictions = clf.predict(X_test_np)\n",
    "acc = accuracy(y_test_np, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
