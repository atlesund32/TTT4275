{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "\n",
    "all_features = [col for col in data.columns if col not in ['Track ID','TrackID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "\n",
    "# Define the features and targets\n",
    "features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'chroma_stft_10_mean']\n",
    "targets = ['Genre']\n",
    "\n",
    "# feature data\n",
    "X_train = train[features]\n",
    "# genre data\n",
    "y_train = train[targets]\n",
    "\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts before implementing \n",
    "\n",
    "Idea: Implement an algorithm that \n",
    "* iterates through each class\n",
    "    * splits the class into M clusters that are used as references for that class\n",
    "\n",
    "\n",
    "So: We have an interative procedure for finding the number of clusters (class references).\n",
    "In order to implement this procedure we need a stopping criteria, since we don't know the exact number beforehand.\n",
    "\n",
    "Start with 1 cluster per class (M=1)\n",
    "calculate\n",
    "* Λ_s (contains the mean and covariance for M=1)\n",
    "* D_1 (the sum of distances between all points in this class and the mean of that class - using Mahalanobis distance or euclidean for example)\n",
    "These D matrices need to be stored somewhere so that we can compare them\n",
    "\n",
    "\n",
    "Then iteratively we increase M by splitting one of the clusters in two.\n",
    "\n",
    "1. Choose which cluster to split - pick the cluster with the largest accumulated distance (D). Or choose the \n",
    "2. The cluster to split is denoted as Λ_s. To split this cluster use a small noise vector w and create two new centers:\n",
    "    * my_s1 = my_s + w \n",
    "    * my_s2 = my_s - w \n",
    "* The covariance matrices are simply copied\n",
    "\n",
    "3. Now for each training point x_k in class w_i\n",
    "    * Compute the distance to all M+1 cluster centers\n",
    "        * d = d(x_k, my_j)\n",
    "    * Assign x_k to the cluster with lowest distance\n",
    "\n",
    "4. Now since we have reassigned our data, we need to recompute the parameters for our clusters\n",
    "    * compute new my_j and covarianceMatrix_j as done earlier\n",
    "\n",
    "\n",
    "5. Evaluate total distance D_M\n",
    "    * Find the accumulated distance and compare with previous D_M-1\n",
    "        * If D_M << D_M-1: continue to next split\n",
    "        * if not stop splitting fo this class -> we are satsified.\n",
    "\n",
    "\n",
    "Summarized:\n",
    "\n",
    "Loop over classes:\n",
    "* Start with 1 cluster\n",
    "* While improvement:\n",
    "    * Find worst cluster\n",
    "    * Split μ with noise ±w\n",
    "    * Assign training vectors\n",
    "    * Recompute μ, Σ\n",
    "    * Compute new total distance\n",
    "    * Check for convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataclass for storing the structured data\n",
    "@dataclass\n",
    "class Cluster:\n",
    "    mean: np.ndarray\n",
    "    covariance: np.ndarray\n",
    "    datapoints: np.ndarray\n",
    "    accumulated_distance: float = 0.0\n",
    "    inv_covariance: np.ndarray = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "# Calculates the Euclidean distance between two music samples\n",
    "def eucledian_distance(datapoint, mu):\n",
    "    distance = 0\n",
    "    for i in range(len(datapoint)):\n",
    "        distance += (datapoint[i] - mu[i]) ** 2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "def calculate_mean(X):\n",
    "    n_samples, n_features = X.shape\n",
    "    accumalated_sum = np.zeros(n_features)\n",
    "    for k in range(n_samples):\n",
    "        accumalated_sum += X[k][:]\n",
    "    \n",
    "    mean = accumalated_sum/n_samples\n",
    "    return mean\n",
    "\n",
    "def calculate_covariance(X, mean):\n",
    "    n_samples, n_features = X.shape\n",
    "    covariance_matrix = np.zeros((n_features, n_features))\n",
    "    for k in range(n_samples):\n",
    "        diff = X[k] - mean\n",
    "        covariance_matrix += np.outer(diff, diff)\n",
    "\n",
    "    covariance_matrix /= n_samples\n",
    "    return covariance_matrix\n",
    "\n",
    "# Calculates the sum of distances between all points in this class and the mean of that class\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accumulated_distance(X, mean, covariance):\n",
    "    '''\n",
    "    Args:\n",
    "        X\n",
    "        mean\n",
    "        covariance\n",
    "    '''\n",
    "    n_samples, _ = X.shape\n",
    "    accumulated_distance = 0.0\n",
    "\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(covariance)\n",
    "    except np.linalg.LinAlgError:\n",
    "        covariance += np.eye(covariance.shape[0]) * 1e-6\n",
    "        inv_cov = np.linalg.inv(covariance)\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        diff = X[k] - mean\n",
    "        distance = diff.T @ inv_cov @ diff  # Mahalanobis squared\n",
    "        accumulated_distance += distance\n",
    "\n",
    "    return accumulated_distance  # total D\n",
    "\n",
    "\n",
    "def find_cluster_to_split(cluster_dict, current_class):\n",
    "    clusters = cluster_dict[current_class]\n",
    "    if(len(clusters) == 1):\n",
    "        return clusters[0]\n",
    "    else:\n",
    "        \n",
    "        cluster_to_split = clusters[0]\n",
    "        for i in range(1, len(clusters)):\n",
    "            if(clusters[i].accumulated_distance > cluster_to_split.accumulated_distance):\n",
    "                cluster_to_split = clusters[i] \n",
    "\n",
    "    return cluster_to_split\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting cluster:\n",
      "  mu = [ 6.57230942e+03 -6.98543422e+01  3.02657942e+03  4.02214269e-01]\n",
      "  w  = [ 5.31715547e+02  2.75079249e+01  5.33149144e+02 -6.53412071e-02]\n",
      "  mu_1 = [ 7.10402497e+03 -4.23464172e+01  3.55972856e+03  3.36873062e-01]\n",
      "  mu_2 = [ 6.04059388e+03 -9.73622671e+01  2.49343027e+03  4.67555476e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 5.07810409e+03 -6.48245061e+01  2.56655673e+03  4.95371905e-01]\n",
      "  w  = [ 1.21147351e+02 -3.11244698e+01 -6.19111193e+01  1.60949183e-02]\n",
      "  mu_1 = [ 5.19925144e+03 -9.59489759e+01  2.50464561e+03  5.11466824e-01]\n",
      "  mu_2 = [ 4.95695673e+03 -3.37000364e+01  2.62846785e+03  4.79276987e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 5.57114500e+03 -9.08886046e+01  2.64650614e+03  4.11966273e-01]\n",
      "  w  = [ 2.69633678e+02 -3.44751707e+01  1.60598482e+02 -6.32407093e-02]\n",
      "  mu_1 = [ 5.84077868e+03 -1.25363775e+02  2.80710462e+03  3.48725564e-01]\n",
      "  mu_2 = [ 5.30151133e+03 -5.64134338e+01  2.48590766e+03  4.75206983e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 3.65012887e+03 -1.66777030e+02  1.72588633e+03  3.59026381e-01]\n",
      "  w  = [ 9.65825976e+02 -3.14109412e+01 -1.31985182e+02 -5.09406870e-02]\n",
      "  mu_1 = [ 4.61595484e+03 -1.98187971e+02  1.59390115e+03  3.08085694e-01]\n",
      "  mu_2 = [ 2.68430289e+03 -1.35366089e+02  1.85787151e+03  4.09967068e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 4.63659853e+03 -1.60358226e+02  2.19024183e+03  4.20697305e-01]\n",
      "  w  = [ 2.41705483e+02  3.47380458e+01  5.25049203e+02 -4.82974539e-02]\n",
      "  mu_1 = [ 4.87830401e+03 -1.25620180e+02  2.71529104e+03  3.72399851e-01]\n",
      "  mu_2 = [ 4.39489304e+03 -1.95096272e+02  1.66519263e+03  4.68994759e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 2.41611039e+03 -3.22684633e+02  1.33059470e+03  2.88243931e-01]\n",
      "  w  = [-5.25229214e+02  6.54399800e+01 -2.54507867e+02  5.81974562e-04]\n",
      "  mu_1 = [ 1.89088118e+03 -2.57244653e+02  1.07608683e+03  2.88825906e-01]\n",
      "  mu_2 = [ 2.94133960e+03 -3.88124613e+02  1.58510256e+03  2.87661957e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 4.60577848e+03 -1.10328491e+02  2.22566077e+03  4.38115274e-01]\n",
      "  w  = [6.57703838e+02 1.44274315e+01 2.30503034e+02 5.66593393e-02]\n",
      "  mu_1 = [ 5.26348232e+03 -9.59010596e+01  2.45616380e+03  4.94774614e-01]\n",
      "  mu_2 = [ 3.94807464e+03 -1.24755923e+02  1.99515773e+03  3.81455935e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 5.29366201e+03 -1.04321033e+02  2.50757399e+03  4.79732267e-01]\n",
      "  w  = [-3.93564100e+02  8.06748545e+00 -4.46499491e+02  8.50899562e-02]\n",
      "  mu_1 = [ 4.90009791e+03 -9.62535478e+01  2.06107449e+03  5.64822223e-01]\n",
      "  mu_2 = [ 5.68722611e+03 -1.12388519e+02  2.95407348e+03  3.94642311e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 3.88417854e+03 -1.46342822e+02  1.87133202e+03  3.84043427e-01]\n",
      "  w  = [ 1.17881293e+03 -4.11052880e+00  2.37280388e+02  6.43503524e-02]\n",
      "  mu_1 = [ 5.06299148e+03 -1.50453351e+02  2.10861241e+03  4.48393780e-01]\n",
      "  mu_2 = [ 2.70536561e+03 -1.42232293e+02  1.63405164e+03  3.19693075e-01]\n",
      "Splitting cluster:\n",
      "  mu = [ 3.71309474e+03 -2.10230883e+02  1.77919734e+03  3.04762813e-01]\n",
      "  w  = [ 2.76393605e+02  5.29703487e+01 -4.07691229e+02  4.25343024e-04]\n",
      "  mu_1 = [ 3.98948835e+03 -1.57260534e+02  1.37150611e+03  3.05188157e-01]\n",
      "  mu_2 = [ 3.43670114e+03 -2.63201231e+02  2.18688857e+03  3.04337470e-01]\n"
     ]
    }
   ],
   "source": [
    "def create_classifier(X_train, y_train, classes, beta):\n",
    "    cluster_dict = {}\n",
    "    for current_class in classes:\n",
    "        # Start with one cluster\n",
    "        M = 1\n",
    "        cluster_dict[current_class] = []\n",
    "        clusters = cluster_dict[current_class]\n",
    "\n",
    "        class_data = X_train[y_train['Genre'] == current_class].to_numpy() # X_train[mask] - selects the row where the mask is true (works because of numpy)\n",
    "\n",
    "        # Calculate the mean of the current cluster\n",
    "        mean = calculate_mean(class_data)\n",
    "        cov = calculate_covariance(class_data, mean)\n",
    "        acc_dist = calculate_accumulated_distance(class_data, mean, cov)\n",
    "        \n",
    "        cluster_main = Cluster(mean=mean, covariance=cov, datapoints=class_data, accumulated_distance=acc_dist)\n",
    "\n",
    "        clusters.append(cluster_main)\n",
    "        we_are_improving = True\n",
    "\n",
    "        previous_accumulated_distance = 0\n",
    "        for cluster in clusters:\n",
    "                previous_accumulated_distance += cluster.accumulated_distance\n",
    "\n",
    "        while(we_are_improving):\n",
    "            \n",
    "            current_accumulated_distance = 0\n",
    "\n",
    "            \n",
    "\n",
    "            splitting_cluster = find_cluster_to_split(cluster_dict, current_class)\n",
    "            mu = splitting_cluster.mean\n",
    "            cov = splitting_cluster.covariance\n",
    "            \n",
    "            # Use the varianve of the splitting cluster to generate a noise vector\n",
    "            \n",
    "            stds = np.sqrt(np.diag(cov))\n",
    "            w = np.random.uniform(low=-stds, high=stds)\n",
    "\n",
    "\n",
    "            # Altered mean for the new two clusters\n",
    "            mu_1 = mu + w\n",
    "            mu_2 = mu - w\n",
    "            cluster_split_1 = Cluster(mean=mu_1, covariance=cov, datapoints=[])\n",
    "            cluster_split_2 = Cluster(mean=mu_2, covariance=cov, datapoints=[])\n",
    "\n",
    "            print(\"Splitting cluster:\")\n",
    "            print(\"  mu =\", mu)\n",
    "            print(\"  w  =\", w)\n",
    "            print(\"  mu_1 =\", mu_1)\n",
    "            print(\"  mu_2 =\", mu_2)\n",
    "\n",
    "\n",
    "\n",
    "            # Update our clusters array\n",
    "            clusters.remove(splitting_cluster)\n",
    "            clusters.append(cluster_split_1)\n",
    "            clusters.append(cluster_split_2)\n",
    "\n",
    "            # Before assigning points to clusters, need to clear the datapoints.\n",
    "            for cluster in clusters:\n",
    "                cluster.datapoints = []\n",
    "\n",
    "\n",
    "            # Assign all datapoints between our new clusters. Necessary since there are new distributions in the collection.\n",
    "\n",
    "            \n",
    "            for x in class_data:\n",
    "                closest_cluster = clusters[0]\n",
    "                closest_distance = eucledian_distance(x, closest_cluster.mean)\n",
    "                for c in range(1, len(clusters)):\n",
    "                    if eucledian_distance(x, clusters[c].mean) < closest_distance:\n",
    "                        closest_distance = eucledian_distance(x, clusters[c].mean)\n",
    "                        closest_cluster = clusters[c]\n",
    "                \n",
    "                closest_cluster.datapoints.append(x)\n",
    "        \n",
    "            # After assigning the datapoints, we need to calculate the new mean, covariance and accumulated distance\n",
    "            for cluster in clusters:\n",
    "                if len(cluster.datapoints) == 0:\n",
    "                    continue \n",
    "                X_cluster = np.array(cluster.datapoints)\n",
    "                cluster.mean = calculate_mean(X_cluster)\n",
    "                cluster.covariance = calculate_covariance(X_cluster, cluster.mean)\n",
    "                cluster.accumulated_distance = calculate_accumulated_distance(X_cluster, cluster.mean, cluster.covariance)\n",
    "\n",
    "                current_accumulated_distance+= cluster.accumulated_distance\n",
    "\n",
    "            \n",
    "            if current_accumulated_distance >= beta* previous_accumulated_distance:\n",
    "                we_are_improving = False\n",
    "            else:\n",
    "                previous_accumulated_distance = current_accumulated_distance\n",
    "\n",
    "\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = y_train['Genre'].unique()\n",
    "#print(f\"Classes: {classes}\")\n",
    "beta = 0.99\n",
    "cluster_dict = create_classifier(X_train, y_train, classes, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop has 2 clusters\n",
      "metal has 2 clusters\n",
      "disco has 2 clusters\n",
      "blues has 2 clusters\n",
      "reggae has 2 clusters\n",
      "classical has 2 clusters\n",
      "rock has 2 clusters\n",
      "hiphop has 2 clusters\n",
      "country has 2 clusters\n",
      "jazz has 2 clusters\n"
     ]
    }
   ],
   "source": [
    "for i in classes:\n",
    "    print(f\"{i} has {len(cluster_dict[i])} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
