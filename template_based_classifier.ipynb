{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "\n",
    "all_features = [col for col in data.columns if col not in ['Track ID','TrackID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "\n",
    "# Define the features and targets\n",
    "features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'chroma_stft_10_mean']\n",
    "targets = ['Genre']\n",
    "\n",
    "# feature data\n",
    "X_train = train[features]\n",
    "# genre data\n",
    "y_train = train[targets]\n",
    "\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts before implementing \n",
    "\n",
    "Idea: Implement an algorithm that \n",
    "* iterates through each class\n",
    "    * splits the class into M clusters that are used as references for that class\n",
    "\n",
    "\n",
    "So: We have an interative procedure for finding the number of clusters (class references).\n",
    "In order to implement this procedure we need a stopping criteria, since we don't know the exact number beforehand.\n",
    "\n",
    "Start with 1 cluster per class (M=1)\n",
    "calculate\n",
    "* Λ_s (contains the mean and covariance for M=1)\n",
    "* D_1 (the sum of distances between all points in this class and the mean of that class - using Mahalanobis distance or euclidean for example)\n",
    "These D matrices need to be stored somewhere so that we can compare them\n",
    "\n",
    "\n",
    "Then iteratively we increase M by splitting one of the clusters in two.\n",
    "\n",
    "1. Choose which cluster to split - pick the cluster with the largest accumulated distance (D). Or choose the \n",
    "2. The cluster to split is denoted as Λ_s. To split this cluster use a small noise vector w and create two new centers:\n",
    "    * my_s1 = my_s + w \n",
    "    * my_s2 = my_s - w \n",
    "* The covariance matrices are simply copied\n",
    "\n",
    "3. Now for each training point x_k in class w_i\n",
    "    * Compute the distance to all M+1 cluster centers\n",
    "        * d = d(x_k, my_j)\n",
    "    * Assign x_k to the cluster with lowest distance\n",
    "\n",
    "4. Now since we have reassigned our data, we need to recompute the parameters for our clusters\n",
    "    * compute new my_j and covarianceMatrix_j as done earlier\n",
    "\n",
    "\n",
    "5. Evaluate total distance D_M\n",
    "    * Find the accumulated distance and compare with previous D_M-1\n",
    "        * If D_M << D_M-1: continue to next split\n",
    "        * if not stop splitting fo this class -> we are satsified.\n",
    "\n",
    "\n",
    "Summarized:\n",
    "\n",
    "Loop over classes:\n",
    "* Start with 1 cluster\n",
    "* While improvement:\n",
    "    * Find worst cluster\n",
    "    * Split μ with noise ±w\n",
    "    * Assign training vectors\n",
    "    * Recompute μ, Σ\n",
    "    * Compute new total distance\n",
    "    * Check for convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataclass for storing the structured data\n",
    "@dataclass\n",
    "class Cluster:\n",
    "    mean: np.ndarray\n",
    "    covariance: np.ndarray\n",
    "    datapoints: np.ndarray\n",
    "    accumulated_distance: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "# Calculates the Euclidean distance between two music samples\n",
    "def eucledian_distance(datapoint, mu):\n",
    "    distance = 0\n",
    "    for i in range(len(datapoint)):\n",
    "        distance += (datapoint[i] - mu[i]) ** 2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "def calculate_mean(X):\n",
    "    n_samples, n_features = X.shape\n",
    "    accumalated_sum = np.zeros(n_features)\n",
    "    print(accumalated_sum)\n",
    "    for k in range(n_samples):\n",
    "        accumalated_sum += X[k][:]\n",
    "    \n",
    "    mean = accumalated_sum/n_samples\n",
    "    return mean\n",
    "\n",
    "def calculate_covariance(X, mean):\n",
    "    n_samples, n_features = X.shape\n",
    "    covariance_matrix = np.zeros((n_features, n_features))\n",
    "    for k in range(n_samples):\n",
    "        diff = X[k] - mean\n",
    "        covariance_matrix += np.outer(diff, diff)\n",
    "\n",
    "    covariance_matrix /= n_samples\n",
    "    return covariance_matrix\n",
    "\n",
    "# Calculates the sum of distances between all points in this class and the mean of that class\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accumulated_distance(X, mean, covariance):\n",
    "    '''\n",
    "    Args:\n",
    "        X\n",
    "        mean\n",
    "        covariance\n",
    "    '''\n",
    "    n_samples, _ = X.shape\n",
    "    accumulated_distance = 0.0\n",
    "\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(covariance)\n",
    "    except np.linalg.LinAlgError:\n",
    "        covariance += np.eye(covariance.shape[0]) * 1e-6\n",
    "        inv_cov = np.linalg.inv(covariance)\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        diff = X[k] - mean\n",
    "        distance = diff.T @ inv_cov @ diff  # Mahalanobis squared\n",
    "        accumulated_distance += distance\n",
    "\n",
    "    return accumulated_distance  # total D\n",
    "\n",
    "\n",
    "def find_cluster_to_split(cluster_dict, current_class):\n",
    "    clusters = cluster_dict[current_class]\n",
    "    if(len(clusters) == 1):\n",
    "        return clusters[0]\n",
    "    else:\n",
    "        \n",
    "        cluster_to_split = clusters[0]\n",
    "        for i in range(1, len(clusters)):\n",
    "            if(clusters[i].accumulated_distance > cluster_to_split.accumulated_distance):\n",
    "                cluster_to_split = clusters[i] \n",
    "\n",
    "    return cluster_to_split\n",
    "    \n",
    "\n",
    "# Not made by me – found online\n",
    "def generate_noise_vector(dim, delta=0.1):\n",
    "    \"\"\"\n",
    "    Generates a noise vector of shape (dim,) where each element is sampled from U(-delta, delta).\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of dimensions (length of the feature vector).\n",
    "        delta (float): Maximum absolute size of the noise (small positive number).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Noise vector of shape (dim,)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=-delta, high=delta, size=dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m classes \u001b[38;5;241m=\u001b[39m y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print(f\"Classes: {classes}\")\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mcreate_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mcreate_classifier\u001b[0;34m(X_train, y_train, classes)\u001b[0m\n\u001b[1;32m      5\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the mean of the current cluster\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mcalculate_mean\u001b[0;34m(X, y, current_class)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(accumalated_sum)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m----> 8\u001b[0m     accumalated_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[:]\n\u001b[1;32m     10\u001b[0m mean \u001b[38;5;241m=\u001b[39m accumalated_sum\u001b[38;5;241m/\u001b[39mn_samples\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def create_classifier(X_train, y_train, classes):\n",
    "    cluster_dict = {}\n",
    "    for current_class in classes:\n",
    "        # Start with one cluster\n",
    "        M = 1\n",
    "        clusters = cluster_dict[current_class]\n",
    "\n",
    "        # Calculate the mean of the current cluster\n",
    "        mean = calculate_mean(X_train)\n",
    "        cov = calculate_covariance(X_train, mean)\n",
    "        acc_dist = calculate_accumulated_distance(X_train, mean, cov)\n",
    "        \n",
    "        cluster_1 = Cluster(mean=mean, covariance=cov, points=X_train, accumulated_distance=acc_dist)\n",
    "\n",
    "        clusters.append(cluster_1)\n",
    "\n",
    "        while(...):\n",
    "            splitting_cluster = find_cluster_to_split(cluster_dict, current_class)\n",
    "            mu = splitting_cluster.mean\n",
    "            w = generate_noise_vector(mu.shape[0], delta=0.1)\n",
    "\n",
    "            mu_1 = mu + w\n",
    "            mu_2 = mu - w\n",
    "            cov = splitting_cluster.covariance\n",
    "\n",
    "            cluster_split_1 = Cluster(mean=mu_1, covariance=cov)\n",
    "            cluster_split_2 = Cluster(mean=mu_2, covariance=cov)\n",
    "\n",
    "\n",
    "            clusters.remove(splitting_cluster)\n",
    "            clusters.append(cluster_split_1)\n",
    "            clusters.append(cluster_split_2)\n",
    "\n",
    "            # Before assigning points to clusters, need to clear the datapoints.\n",
    "            for cluster in clusters:\n",
    "                cluster.datapoints = []\n",
    "\n",
    "            class_data = X_train[y_train['Genre'] == current_class] # X_train[mask] - selects the row where the mask is true (works because of numpy)\n",
    "            for x in class_data:\n",
    "                closest_cluster = clusters[0]\n",
    "                closest_distance = eucledian_distance(x, closest_cluster.mean)\n",
    "                for c in range(1, len(clusters)):\n",
    "                    if eucledian_distance(x, clusters[c].mean) < closest_distance:\n",
    "                        closest_distance = eucledian_distance(x, clusters[c].mean)\n",
    "                        closest_cluster = clusters[c]\n",
    "                \n",
    "                closest_cluster.datapoints.append(x)\n",
    "\n",
    "\n",
    "        \n",
    "            for cluster in clusters:\n",
    "                cluster.mean = calculate_mean(X_train)\n",
    "                cluster.covariance = calculate_covariance(X_train, cluster.mean)\n",
    "                cluster.accumulated_distance = calculate_accumulated_distance(X_train, cluster.mean, cluster.covariance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = y_train['Genre'].unique()\n",
    "#print(f\"Classes: {classes}\")\n",
    "create_classifier(X_train, y_train, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
