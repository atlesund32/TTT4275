{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "\n",
    "#all_features = [col for col in data.columns if col not in ['Track ID','TrackID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "'''all_features = [\n",
    "    'zero_cross_rate_mean','zero_cross_rate_std','rmse_mean','rmse_var',\n",
    "    'spectral_centroid_mean','spectral_centroid_var','spectral_bandwidth_mean','spectral_bandwidth_var',\n",
    "    'spectral_rolloff_mean','spectral_rolloff_var','spectral_contrast_mean','spectral_contrast_var',\n",
    "    'spectral_flatness_mean','spectral_flatness_var',\n",
    "    'chroma_stft_1_mean','chroma_stft_2_mean','chroma_stft_3_mean','chroma_stft_4_mean',\n",
    "    'chroma_stft_5_mean','chroma_stft_6_mean','chroma_stft_7_mean','chroma_stft_8_mean',\n",
    "    'chroma_stft_9_mean','chroma_stft_10_mean','chroma_stft_11_mean','chroma_stft_12_mean',\n",
    "    'chroma_stft_1_std','chroma_stft_2_std','chroma_stft_3_std','chroma_stft_4_std',\n",
    "    'chroma_stft_5_std','chroma_stft_6_std','chroma_stft_7_std','chroma_stft_8_std',\n",
    "    'chroma_stft_9_std','chroma_stft_10_std','chroma_stft_11_std','chroma_stft_12_std',\n",
    "    'tempo',\n",
    "    'mfcc_1_mean','mfcc_2_mean','mfcc_3_mean','mfcc_4_mean','mfcc_5_mean','mfcc_6_mean',\n",
    "    'mfcc_7_mean','mfcc_8_mean','mfcc_9_mean','mfcc_10_mean','mfcc_11_mean','mfcc_12_mean',\n",
    "    'mfcc_1_std','mfcc_2_std','mfcc_3_std','mfcc_4_std','mfcc_5_std','mfcc_6_std',\n",
    "    'mfcc_7_std','mfcc_8_std','mfcc_9_std','mfcc_10_std','mfcc_11_std','mfcc_12_std'\n",
    "]'''\n",
    "all_features = [\n",
    "    'zero_cross_rate_mean','zero_cross_rate_std','rmse_mean','rmse_var',\n",
    "    'spectral_centroid_mean','spectral_centroid_var','spectral_bandwidth_mean','spectral_bandwidth_var',\n",
    "    'spectral_rolloff_mean','spectral_rolloff_var','spectral_contrast_mean','spectral_contrast_var',\n",
    "    'spectral_flatness_mean','spectral_flatness_var',\n",
    "    'chroma_stft_7_mean',\n",
    "    \n",
    "    'tempo',\n",
    "    'mfcc_1_mean','mfcc_2_mean','mfcc_3_mean','mfcc_4_mean','mfcc_5_mean','mfcc_6_mean',\n",
    "\n",
    "    'mfcc_2_std','mfcc_3_std','mfcc_4_std','mfcc_5_std', 'mfcc_7_std'\n",
    "]\n",
    "#Erfaringer: \n",
    "#'chroma_stft_x_std' er elendig, drar ned accuracy\n",
    "features = all_features\n",
    "\n",
    "#features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'chroma_stft_10_mean']\n",
    "\n",
    "targets = ['Genre']\n",
    "\n",
    "# feature data\n",
    "X_train, y_train = train[features], train[targets]\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts before implementing \n",
    "\n",
    "Idea: Implement an algorithm that use the training data to\n",
    "* iterate through each class\n",
    "    * split the class into M clusters that are used as references for that class\n",
    "\n",
    "\n",
    "So: We have an interative procedure for finding the number of clusters (class references).\n",
    "In order to implement this procedure we need a stopping criteria, since we don't know the number of clusters that will give the best performance beforehand.\n",
    "\n",
    "Starting with 1 cluster per class (M=1) we calculate\n",
    "\n",
    "* Λ_s (contains the mean and covariance as a tuple)\n",
    "* D_1 (the sum of distances between all points in this class and the mean of that class - using Mahalanobis distance or euclidean for example)\n",
    "These D matrices need to be stored somewhere so that we can compare them\n",
    "\n",
    "\n",
    "Then iteratively we increase M by splitting one of the clusters in two.\n",
    "\n",
    "1. Choose which cluster to split - pick the cluster with the largest accumulated distance (D). Or choose the \n",
    "2. The cluster to split is denoted as Λ_s. To split this cluster use a small noise vector w and create two new centers:\n",
    "    * my_s1 = my_s + w \n",
    "    * my_s2 = my_s - w \n",
    "* The covariance matrices are simply copied\n",
    "\n",
    "3. Now for each training point x_k in class w_i\n",
    "    * Compute the distance to all M+1 cluster centers\n",
    "        * d = d(x_k, my_j)\n",
    "    * Assign x_k to the cluster with lowest distance\n",
    "\n",
    "4. Now since we have reassigned our data, we need to recompute the parameters for our clusters\n",
    "    * compute new my_j and covarianceMatrix_j as done earlier\n",
    "\n",
    "\n",
    "5. Evaluate total distance D_M\n",
    "    * Find the accumulated distance and compare with previous D_M-1\n",
    "        * If D_M << D_M-1: continue to next split\n",
    "        * if not stop splitting fo this class -> we are satsified.\n",
    "\n",
    "\n",
    "Summarized:\n",
    "\n",
    "Loop over classes:\n",
    "* Start with 1 cluster\n",
    "* While improvement:\n",
    "    * Find worst cluster\n",
    "    * Split μ with noise ±w\n",
    "    * Assign training vectors\n",
    "    * Recompute μ, Σ\n",
    "    * Compute new total distance\n",
    "    * Check for convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_train['Genre'], palette='bright')\n",
    "plt.title(\"PCA visualization of genres\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataclass for storing the structured data\n",
    "@dataclass(eq=False)\n",
    "class Cluster:\n",
    "    mean: np.ndarray\n",
    "    covariance: np.ndarray\n",
    "    datapoints: np.ndarray\n",
    "    accumulated_distance: float = 0.0\n",
    "    inv_covariance: np.ndarray = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "# Calculates the Euclidean distance between two music samples\n",
    "def eucledian_distance(datapoint, mu):\n",
    "    distance = 0\n",
    "    for i in range(len(datapoint)):\n",
    "        distance += (datapoint[i] - mu[i]) ** 2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "def calculate_mean(X):\n",
    "    n_samples, n_features = X.shape\n",
    "    accumalated_sum = np.zeros(n_features)\n",
    "    for k in range(n_samples):\n",
    "        accumalated_sum += X[k][:]\n",
    "    \n",
    "    mean = accumalated_sum/n_samples\n",
    "    return mean\n",
    "\n",
    "def calculate_covariance(X, mean):\n",
    "    n_samples, n_features = X.shape\n",
    "    covariance_matrix = np.zeros((n_features, n_features))\n",
    "    for k in range(n_samples):\n",
    "        diff = X[k] - mean\n",
    "        covariance_matrix += np.outer(diff, diff)\n",
    "\n",
    "    covariance_matrix /= n_samples\n",
    "    return covariance_matrix\n",
    "\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        cov += np.eye(cov.shape[0]) * 1e-6\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "    diff = x - mean\n",
    "    return diff.T @ inv_cov @ diff\n",
    "\n",
    "\n",
    "# Calculates the sum of distances between all points in this class and the mean of that class\n",
    "def calculate_accumulated_distance(X, mean, covariance):\n",
    "    '''\n",
    "    Args:\n",
    "        X\n",
    "        mean\n",
    "        covariance\n",
    "    '''\n",
    "    n_samples, _ = X.shape\n",
    "    accumulated_distance = 0.0\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        distance = mahalanobis_distance(X[k], mean, covariance)\n",
    "        accumulated_distance += distance\n",
    "\n",
    "    return accumulated_distance  # total D\n",
    "\n",
    "\n",
    "def find_cluster_to_split_distance(cluster_dict, current_class):\n",
    "    \"\"\"\n",
    "    Return the cluster in `cluster_dict[current_class]` that has\n",
    "    the largest accumulated distance.\n",
    "    \"\"\"\n",
    "    clusters = cluster_dict[current_class]\n",
    "    if(len(clusters) == 1):\n",
    "        return clusters[0]\n",
    "    else:\n",
    "        \n",
    "        cluster_to_split = clusters[0]\n",
    "        for i in range(1, len(clusters)):\n",
    "            if(clusters[i].accumulated_distance > cluster_to_split.accumulated_distance):\n",
    "                cluster_to_split = clusters[i] \n",
    "\n",
    "    return cluster_to_split\n",
    "\n",
    "def find_cluster_to_split_points(cluster_dict, current_class):\n",
    "    \"\"\"\n",
    "    Return the cluster in `cluster_dict[current_class]` that has\n",
    "    the largest number of datapoints.\n",
    "    \"\"\"\n",
    "    clusters = cluster_dict[current_class]\n",
    "    \n",
    "    if len(clusters) == 1:\n",
    "        return clusters[0]\n",
    "    # otherwise pick the cluster with the maximum number of datapoints\n",
    "    # (datapoints may be a numpy array or a list)\n",
    "    return max(clusters, key=lambda c: len(c.datapoints))\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(X_train, y_train, classes):\n",
    "    \n",
    "    cluster_dict = {}\n",
    "    for current_class in classes:\n",
    "        counter = 1\n",
    "        # Start with one cluster\n",
    "        cluster_dict[current_class] = []\n",
    "        clusters = cluster_dict[current_class]\n",
    "\n",
    "        class_data = X_train[y_train['Genre'] == current_class].to_numpy() # X_train[mask] - selects the row where the mask is true (works because of numpy)\n",
    "\n",
    "        # Calculate the mean of the current cluster\n",
    "        mean = calculate_mean(class_data)\n",
    "        cov = calculate_covariance(class_data, mean)\n",
    "        acc_dist = calculate_accumulated_distance(class_data, mean, cov)\n",
    "        \n",
    "        cluster_main = Cluster(mean=mean, covariance=cov, datapoints=class_data, accumulated_distance=acc_dist)\n",
    "\n",
    "        clusters.append(cluster_main)\n",
    "        we_are_improving = True\n",
    "\n",
    "        previous_accumulated_distance = 0\n",
    "        for cluster in clusters:\n",
    "                previous_accumulated_distance += cluster.accumulated_distance\n",
    "        #or counter < 2:\n",
    "        '''while we_are_improving:  # Removing this entire while loop increases accuracy...\n",
    "            counter += 1\n",
    "            \n",
    "            current_accumulated_distance = 0\n",
    "\n",
    "        \n",
    "            splitting_cluster = find_cluster_to_split_points(cluster_dict, current_class)\n",
    "            mu = splitting_cluster.mean\n",
    "            cov = splitting_cluster.covariance\n",
    "            \n",
    "            # Use the varianve of the splitting cluster to generate a noise vector\n",
    "            \n",
    "             \n",
    "            stds = np.sqrt(np.diag(cov))\n",
    "            # w = np.random.uniform(low=-stds, high=stds)\n",
    "            # i stedet for uniform(-stds, stds)\n",
    "            scale_factor = 5.0\n",
    "            w = scale_factor * np.random.uniform(-stds, stds)\n",
    "\n",
    "\n",
    "\n",
    "            # Altered mean for the new two clusters\n",
    "            mu_1 = mu + w\n",
    "            mu_2 = mu - w\n",
    "            cluster_split_1 = Cluster(mean=mu_1, covariance=cov, datapoints=[])\n",
    "            cluster_split_2 = Cluster(mean=mu_2, covariance=cov, datapoints=[])\n",
    "\n",
    "            print(\"Splitting cluster:\")\n",
    "            print(\"  mu =\", mu)\n",
    "            print(\"  w  =\", w)\n",
    "            print(\"  mu_1 =\", mu_1)\n",
    "            print(\"  mu_2 =\", mu_2)\n",
    "\n",
    "\n",
    "\n",
    "            # Update our clusters array\n",
    "            clusters.remove(splitting_cluster)\n",
    "            clusters.append(cluster_split_1)\n",
    "            clusters.append(cluster_split_2)\n",
    "\n",
    "            # Before assigning points to clusters, need to clear the datapoints.\n",
    "            for cluster in clusters:\n",
    "                cluster.datapoints = []\n",
    "\n",
    "\n",
    "            # Assign all datapoints between our new clusters. Necessary since there are new distributions in the collection.\n",
    "\n",
    "            \n",
    "            for x in class_data:\n",
    "                closest_cluster = clusters[0]\n",
    "                closest_distance = eucledian_distance(x, closest_cluster.mean)\n",
    "                for c in range(1, len(clusters)):\n",
    "                    if eucledian_distance(x, clusters[c].mean) < closest_distance:\n",
    "                        closest_distance = eucledian_distance(x, clusters[c].mean)\n",
    "                        closest_cluster = clusters[c]\n",
    "                \n",
    "                closest_cluster.datapoints.append(x)\n",
    "        \n",
    "            # After assigning the datapoints, we need to calculate the new mean, covariance and accumulated distance\n",
    "            for cluster in clusters:\n",
    "                if len(cluster.datapoints) == 0:\n",
    "                    continue \n",
    "                X_cluster = np.array(cluster.datapoints)\n",
    "                cluster.mean = calculate_mean(X_cluster)\n",
    "                cluster.covariance = calculate_covariance(X_cluster, cluster.mean)\n",
    "                cluster.accumulated_distance = calculate_accumulated_distance(X_cluster, cluster.mean, cluster.covariance)\n",
    "\n",
    "                current_accumulated_distance += cluster.accumulated_distance\n",
    "\n",
    "            \n",
    "            if current_accumulated_distance >= beta* previous_accumulated_distance:\n",
    "                we_are_improving = False\n",
    "            else:\n",
    "                previous_accumulated_distance = current_accumulated_distance'''\n",
    "\n",
    "\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = y_train['Genre'].unique()\n",
    "#print(f\"Classes: {classes}\")\n",
    "beta = 0.99\n",
    "cluster_dict = create_classifier(X_train, y_train, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in classes:\n",
    "    print(f\"{i} has {len(cluster_dict[i])} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_for_class(class_name, cluster_dict, X_train, y_train):\n",
    "    class_data = X_train[y_train['Genre'] == class_name]\n",
    "\n",
    "    \n",
    "    pca = PCA(n_components=2) #Reduces to 2 dimensions for plotting\n",
    "    class_data_2d = pca.fit_transform(class_data) # Fit and transform the data to 2D\n",
    "\n",
    "    # Transform cluster-means\n",
    "    cluster_means_2d = [pca.transform(cluster.mean.reshape(1, -1))[0] for cluster in cluster_dict[class_name]]\n",
    "\n",
    "    # Plot datapunktene\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(class_data_2d[:, 0], class_data_2d[:, 1], c='lightgray', label='Data points')\n",
    "\n",
    "    # Plot cluster sentrene\n",
    "    for idx, mean_2d in enumerate(cluster_means_2d):\n",
    "        plt.scatter(mean_2d[0], mean_2d[1], c='red', marker='X', s=100, label=f'Cluster' if idx == 0 else \"\")\n",
    "    \n",
    "    plt.title(f\"Clusters for class '{class_name}'\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_for_class('blues', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('classical', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('country', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('disco', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('hiphop', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('jazz', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('metal', cluster_dict, X_train, y_train)\n",
    "plot_clusters_for_class('pop', cluster_dict, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sample(x,cluster_dict):\n",
    "    best_distance = float('inf')\n",
    "    predicted_class = ''\n",
    "\n",
    "    for class_name, clusters in cluster_dict.items(): # For key, values in dict\n",
    "        for cluster in clusters:\n",
    "            d = mahalanobis_distance(x,cluster.mean, cluster.covariance)\n",
    "            if d < best_distance:\n",
    "                best_distance = d\n",
    "                predicted_class = class_name\n",
    "    return predicted_class\n",
    "    \n",
    "\n",
    "predicted_label = classify_sample(X_test.iloc[0],cluster_dict)\n",
    "print(f\"Predicted label: {predicted_label}. True label: {y_test.iloc[0]['Genre']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, cluster_dict):\n",
    "    predictions = []\n",
    "    for x in X_test.to_numpy():\n",
    "        label = classify_sample(x, cluster_dict)\n",
    "        predictions.append(label)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "y_test_true = y_test['Genre'].to_numpy()\n",
    "y_pred = predict(X_test, cluster_dict)\n",
    "\n",
    "accuracy = accuracy_score(y_test_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "classes = np.unique(y_test_true)\n",
    "\n",
    "print(\"Overall accuracy:   \", accuracy_score(y_test, y_pred))\n",
    "print()\n",
    "print(\"Per‐class performance:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    labels=classes,       \n",
    "    target_names=classes,\n",
    "    digits=4\n",
    "))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(\n",
    "    y_test_true,\n",
    "    y_pred,\n",
    "    labels=classes\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liten konklusjon:\n",
    "\n",
    "Music often has overlapping properties with large variations. Classification of 10 genres is a demanding a task and a simple clustering approach may not be sufficient.\n",
    "\n",
    "Also, by splitting a parent cluster into two \"baby-clusters\" that are simply shifted with a noise vector we are hurting the class-separator. We should instead have the splits sit on the actual peaks of the data density, and not on random perturbations.\n",
    "from split & hope\n",
    "\n",
    "List of improvements:\n",
    "FIRST: Replace the splitting logic with GMM + EM\n",
    "1) Normalize the data - we need to scale the data so that they follow a simialr size \n",
    "2) Evaluer klyngene\n",
    "    Plott klyngene i 2D (PCA) og se om datapunktene i hver klynge virkelig hører til den tilsiktede sjangeren, eller om mange punkter er åpenbart “feilplassert”.\n",
    "3) For å finne ut om teknikken virkelig passer, bør du sammenligne med en standard klassifikator (f.eks. SVM, RF, MLP) og også undersøke parametere (antall klynger, normalisering, robust estimering av kovarianser, osv.). Hvis du raskt kan få bedre resultater med mer vanlige supervised-metoder, er det et tydelig tegn på at kluster-tilnærmingen ikke er særlig godt egnet eller i det minste krever langt mer avanserte justeringer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
