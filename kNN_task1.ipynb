{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "\n",
    "all_features = [col for col in data.columns if col not in ['Track ID','TrackID', 'File', 'GenreID', 'Genre', 'Type']]\n",
    "\n",
    "# Define the features and targets\n",
    "features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'chroma_stft_10_mean']\n",
    "targets = ['Genre']\n",
    "\n",
    "# feature data\n",
    "X_train = train[features]\n",
    "# genre data\n",
    "y_train = train[targets]\n",
    "\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # For training set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the Euclidean distance between two music samples\n",
    "def eucledian_distance(sample_1, sample_2):\n",
    "    distance = 0\n",
    "    for i in range(len(sample_1)):\n",
    "        distance += (sample_1[i] - sample_2[i]) ** 2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "# Get feature matrix including genre column\n",
    "def get_feature_matrix(X_train_array, y_train_df):\n",
    "\n",
    "    # y_train_df is a pandas DataFrame with shape (n, 1)\n",
    "    genre_column = y_train_df.values.reshape(-1, 1)  # Convert to Nx1 numpy array\n",
    "\n",
    "    # X_train_array is already a numpy array from StandardScaler.\n",
    "    # Using hstack (horizontal stack) to stack the genre column together with the features\n",
    "    feature_matrix_with_genre = np.hstack((genre_column, X_train_array))\n",
    "    return feature_matrix_with_genre\n",
    "\n",
    "\n",
    "# Compute distances and find k nearest neighbors\n",
    "def compute_distances_return_label(feature_matrix, X_test_sample, k):\n",
    "    distances = []\n",
    "\n",
    "    # For each sample in the feature matrix,\n",
    "    for i in range(len(feature_matrix)):\n",
    "        \n",
    "        # Calculate the Euclidean distance between the test sample and the training sample\n",
    "        distance = eucledian_distance(feature_matrix[i][1:], X_test_sample)  # Ignore genre column\n",
    "\n",
    "        # Append the the distance between the training sample and the test sample together with the genre as a tuple\n",
    "        distances.append((feature_matrix[i][0], distance))  # Append genre and distance\n",
    "\n",
    "    # Sort the distances and get the k closest samples\n",
    "    sorted_distances = sorted(distances, key=lambda x: x[1])  # Sort by distance\n",
    "    k_closest_labels = sorted_distances[:k]\n",
    "\n",
    "\n",
    "    labels = [label for label, _ in k_closest_labels]  # Extract the k closest labels\n",
    "    count = Counter(labels)\n",
    "    most_common_label = count.most_common(1)[0][0]\n",
    "    return most_common_label\n",
    "\n",
    "# Get k-NN predictions for the entire test set\n",
    "def get_kNN(k, X_train_array, y_train_df, X_test_array):\n",
    "    feature_matrix = get_feature_matrix(X_train_array, y_train_df)\n",
    "    label_vector = []\n",
    "\n",
    "    # X_test_array is a NumPy array, so we use X_test_array[i]\n",
    "    for i in range(len(X_test_array)):\n",
    "        label_vector.append(compute_distances_return_label(feature_matrix, X_test_array[i], k))\n",
    "\n",
    "    return label_vector\n",
    "\n",
    "# Get the predictions for the test set\n",
    "predicted_labels = get_kNN(5, X_train_scaled, y_train, X_test_scaled)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def knn_feature_selection(candidate_features, current_features, train, test, y_train, y_test, k=5): ##Idiotisk funksjon som sjekker alle features opp mot de som er i features og returnere den som gir best accuracy \n",
    "    best_accuracy = 0\n",
    "    best_feature = None\n",
    "    # Iterate over each candidate that is not already selected\n",
    "    for candidate in candidate_features:\n",
    "        # Form a new feature set: current_features + candidate\n",
    "        new_features = current_features + [candidate]\n",
    "        print(f\"Testing features: {new_features}\")\n",
    "        \n",
    "        # Extract these features for training and testing\n",
    "        X_train_candidate = train[new_features]\n",
    "        X_test_candidate = test[new_features]\n",
    "        \n",
    "        # Scale these feature sets\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled_candidate = scaler.fit_transform(X_train_candidate)\n",
    "        X_test_scaled_candidate = scaler.transform(X_test_candidate)\n",
    "        \n",
    "        # Run k-NN with the current candidate feature set\n",
    "        predicted = get_kNN(k, X_train_scaled_candidate, y_train, X_test_scaled_candidate)\n",
    "        # Convert actual y_test to a 1D array\n",
    "        y_true = y_test.values.flatten()\n",
    "        acc = accuracy_metric(y_true, predicted)\n",
    "        print(f\"Candidate feature '{candidate}' produced accuracy: {acc:.2f}%\")\n",
    "        \n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_feature = candidate\n",
    "        new_features = np.delete(new_features,len(new_features)-1)\n",
    "\n",
    "    print(\"\\nBest candidate feature:\", best_feature, \"with accuracy:\", best_accuracy)\n",
    "    return best_feature, best_accuracy\n",
    "#candidate_features = [f for f in all_features if f not in features]\n",
    "#print(knn_feature_selection(candidate_features,features,train,test,y_train,y_test, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric(y_test.values.flatten(), predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def covariance(X_test,y_test):\n",
    "# print(X_test)\n",
    "\n",
    "\n",
    "\n",
    "def sample_mean(data):\n",
    "    total = 0\n",
    "    for value in range(len(data)):\n",
    "        total += data[value]\n",
    "    return  total/len(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cov(x_data, y_data):\n",
    "    cov_sum = 0\n",
    "    x_mean = sample_mean(x_data)\n",
    "    y_mean = sample_mean(y_data)\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        cov_sum += (x_data[i]-x_mean)*(y_data[i]-y_mean)\n",
    "    return cov_sum\n",
    "\n",
    "\n",
    "\n",
    "def pearson_correlation(x_data, y_data):\n",
    "    sigX = 0\n",
    "    sigY = 0\n",
    "    x_data_mean = sample_mean(x_data)\n",
    "    y_data_mean = sample_mean(y_data)\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        sigX += (x_data[i] - x_data_mean)**2\n",
    "        sigY += (y_data[i] - y_data_mean)**2\n",
    "\n",
    "    return cov(x_data, y_data)/math.sqrt(sigX*sigY)\n",
    "\n",
    "\n",
    "# def get_feature_correlation_matrix(features, train_data):\n",
    "#     corr_matrix = []\n",
    "#     for i in range(len(features)):\n",
    "#         row = []\n",
    "#         f_i = train_data[features[i]]\n",
    "#         for j in range(len(features)):\n",
    "#             f_j = train_data[features[j]]\n",
    "#             row.append(pearson_correlation(f_i,f_j))\n",
    "#         corr_matrix.append(row)\n",
    "#     return corr_matrix\n",
    "\n",
    "def get_feature_correlation_matrix(features, train_data):\n",
    "    corr_matrix = train_data[features].corr()  # Use pandas .corr() for efficient computation\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "def p_corr_features(features, y_data, train_data):\n",
    "    p_vec = []\n",
    "    for i in range(len(features)):\n",
    "        x = train_data[features[i]]\n",
    "        p = pearson_correlation(x,y_data)\n",
    "        p_vec.append(p)\n",
    "    \n",
    "    return p_vec\n",
    "\n",
    "# X_train_test = X_train[features[3]]\n",
    "genre_id = train[\"GenreID\"]\n",
    "\n",
    "\n",
    "corr= p_corr_features(all_features, genre_id, train)\n",
    "\n",
    "corr_matrix = get_feature_correlation_matrix(features, train)\n",
    "# print(corr_matrix)\n",
    "\n",
    "plt.figure(figsize=(14, 6))  # Set the size of the plot\n",
    "plt.bar(all_features, corr, color='skyblue')  # Bar chart\n",
    "plt.xlabel('Features')  # Label for the x-axis\n",
    "plt.ylabel('Pearson Correlation')  # Label for the y-axis\n",
    "plt.title('Pearson Correlation Between Features and GenreID')  # Title\n",
    "plt.xticks(rotation=90)  # Rotate feature names if they are long\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'corr_matrix' is the pandas DataFrame that holds the correlation matrix\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 8))  # Adjust the size of the plot for better readability\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "plt.title('Feature Correlation Heatmap')  # Title of the heatmap\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Korrelasjonsmatrise mellom features (heatmap) og korrelasjon mellom feature og output over**\n",
    "Testet litt forskjellig og kom frem til at det var best å droppe 'tempo' da denne både har lav korrelasjon med output (0.025) og har lav varians. Det kan argumenteres for å også droppe 'spectral_centroid_mean' da denne har korrelerer svært med 'spectral_rolloff_mean' (0.98). Ideelt sett skulle vi ha droppet både denne og tempo.\n",
    "\n",
    "Videre er et forsøk på å finne den neste featuren ved å finne en feature som både korrelerer godt med output og lite med de andre featuresene. Her har jeg forsøkt å hente ut de featuresene som korrelerer mest med output og lagret de i most_correlated. Deretter har jeg plottet disse i et heatmap med de allerede valgte featuresene. Utifra dette kom jeg frem til at zero_cross_rate_mean ga best resultat. da den har god p-korrelasjon og korrelerer bare delvis med de andre featuresene. \n",
    "\n",
    "Jeg skalerte også X_train og X_test da resultatet nesten ikke endret seg hadde jeg ikke skalert (skjønte ikke helt hvorfor, men chatten sa det... Tror det er fordi en av featurene dominerer altfor mye til at de andre har noe å si. Tipper dette kan være spectral_roloff_mean da den hadde svært stor varians.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##find next feature.\n",
    "\n",
    "corr_y = np.abs(p_corr_features(all_features, genre_id, train))\n",
    "\n",
    "most_correlated = [] #de 7 featuresene som korrelerer mest med output\n",
    "\n",
    "for i in range(7):\n",
    "    mostly_correlated = np.argmax(corr_y)\n",
    "    feat = all_features[mostly_correlated]\n",
    "    most_correlated.append([np.max(corr_y), feat])\n",
    "\n",
    "\n",
    "    corr_y = np.delete(corr_y,mostly_correlated)\n",
    "features_test = features\n",
    "for i in range(len(most_correlated)):\n",
    "    if(most_correlated[i][1] not in features):\n",
    "        features_test.append(most_correlated[i][1])\n",
    "\n",
    "print(most_correlated)\n",
    "\n",
    "corr_test = get_feature_correlation_matrix(features_test, train)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the size of the plot for better readability\n",
    "sns.heatmap(corr_test, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "plt.title('Feature Correlation Heatmap')  # Title of the heatmap\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print predicted_labels and y_test together in a table\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(pd.DataFrame({'Predicted': predicted_labels, 'True': y_test.values.ravel()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
