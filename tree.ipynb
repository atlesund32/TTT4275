{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*, value=None): #value will only be passed to leaf nodes\n",
    "        # Parameters after ,*, are keyword-arguments only, has to be addressed as value=42, and not (...,42)\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold # The split value for the feature\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=10, n_features=None):\n",
    "        self.min_samples_split = min_samples_split # Minimum number of samples required to split an internal node, prevents overfitting\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features \n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X,y)\n",
    "\n",
    "    # Recursive function to grow the tree\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Check if we have reached the stopping criteria\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            n_labels == 1):\n",
    "\n",
    "\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        subset_features = np.random.choice(n_features, self.n_features, replace=False) # Only use a subset of unique features\n",
    "\n",
    "        # Find the best split\n",
    "        best_thresh, best_feature = self._best_split(X, y, subset_features)\n",
    "        \n",
    "        # Create the left and right subtrees\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:,best_feature],best_thresh)\n",
    "\n",
    "        if len(left_idxs)  == 0 or len(right_idxs) == 0:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    # Finding the best splits and thresholds \n",
    "    def _best_split(self, X, y, subset_features):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feature in subset_features:\n",
    "            X_column = X[:, feature]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature\n",
    "                    split_thresh = threshold\n",
    "        return split_thresh, split_idx\n",
    "\n",
    "\n",
    "    def _information_gain(self,y,X_column, threshold):\n",
    "        # IG = E(parent) - [weighted average] * E(children)\n",
    "\n",
    "        #parent entropy\n",
    "\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        #create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if(len(left_idxs) == 0 or len(right_idxs) == 0):\n",
    "            return 0\n",
    "\n",
    "        #calculate the avg. weighted entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
    "\n",
    "        #calculate the IG\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column<=split_thresh).flatten() # gives the index of the arguments that fulfills the args. flatten() flattens the list of lists into a list\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    def _entropy(self,y):\n",
    "        # E = - Sum(p(X)*log_2(p(X)))\n",
    "        hist = np.bincount(y) #creates a historgram [instances_of_0, instances_of_1, ...]\n",
    "        ps = hist/len(y)\n",
    "        return -np.sum([p*np.log(p) for p in ps if p>0]) \n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # Return the most common label in y\n",
    "        counter = Counter(y) # REPLACE\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x,node.left)\n",
    "        return self._traverse_tree(x,node.right)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x,self.root) for x in X])\n",
    "    \n",
    "    \n",
    "    # from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "# BaseEstimator, ClassifierMixin\n",
    "class RandomForest():\n",
    "    def __init__(self, n_trees=200, max_depth=10, min_samples_split=3, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "\n",
    "    def fit(self, X,y):\n",
    "\n",
    "        n_tot_features = X.shape[1]\n",
    "        if self.n_features is None:\n",
    "            self.n_features = int(np.sqrt(n_tot_features))\n",
    "        else:\n",
    "            self.n_features = min(self.n_features, n_tot_features) \n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth,\n",
    "                        min_samples_split=self.min_samples_split,\n",
    "                         n_features=self.n_features)\n",
    "            X_sample, y_sample =self._bootstrap_samples(X,y)\n",
    "            tree.fit(X_sample,y_sample)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def _bootstrap_samples(self,X,y):\n",
    "        n_samples, _ = X.shape\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True) # sampling with replacement\n",
    "        return X[idxs], y[idxs]\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        # Return the most common label in y\n",
    "        counter = Counter(y) # REPLACE\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees]) # [[pred_1st_sample_by_1st_tree, pred_2st_sample_by_1st_tree],\n",
    "                                                                        #   [pred_1st_sample_by_2nd_tree],[pred_2nd_sample_by_2nd_tree],\n",
    "                                                                        #   [...], ...]\n",
    "        #[[all predictions of first sample], [all predictions of second sample], [...], ...]\n",
    "\n",
    "        tree_predictions = np.swapaxes(predictions, 0, 1)\n",
    "        predictions = np.array([self._most_common_label(pred) for pred in tree_predictions])\n",
    "        return predictions\n",
    "    \n",
    "    def feature_importance(self, feature_names=None):\n",
    "        \"\"\"\n",
    "        Calculate feature importance based on how often features are used for splitting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        feature_names : list, optional\n",
    "            List of feature names, if None, will use indices\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with feature importances\n",
    "        \"\"\"\n",
    "        feature_counts = {}\n",
    "        \n",
    "        def count_feature_usage(node, counts):\n",
    "            if node is None or node.is_leaf_node():\n",
    "                return\n",
    "            counts[node.feature] = counts.get(node.feature, 0) + 1\n",
    "            count_feature_usage(node.left, counts)\n",
    "            count_feature_usage(node.right, counts)\n",
    "        \n",
    "        # Count feature usage across all trees\n",
    "        for tree in self.trees:\n",
    "            counts = {}\n",
    "            count_feature_usage(tree.root, counts)\n",
    "            \n",
    "            # Normalize counts for each tree\n",
    "            total = sum(counts.values()) or 1\n",
    "            for feature, count in counts.items():\n",
    "                feature_counts[feature] = feature_counts.get(feature, 0) + count / total\n",
    "        \n",
    "        # Average across all trees\n",
    "        n_trees = len(self.trees)\n",
    "        importances = {feature: count / n_trees for feature, count in feature_counts.items()}\n",
    "        \n",
    "        # Create DataFrame\n",
    "        if feature_names is not None:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': [feature_names[feature] for feature in importances.keys()],\n",
    "                'Importance': list(importances.values())\n",
    "            })\n",
    "        else:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': list(importances.keys()),\n",
    "                'Importance': list(importances.values())\n",
    "            })\n",
    "        \n",
    "        return importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Add confusion matrix function\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None):\n",
    "    \"\"\"Plot confusion matrix for the classifier results\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i, acc in enumerate(class_acc):\n",
    "        class_name = class_names[i] if class_names is not None else i\n",
    "        print(f\"Class {class_name}: {acc:.4f}\")\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Feature selection using correlation analysis\n",
    "def select_features(X, y, features, threshold=0.7, top_n=None):\n",
    "    \"\"\"Select features based on correlation with target and between features\"\"\"\n",
    "    # Create DataFrame with features and target\n",
    "    data = pd.DataFrame(X, columns=features)\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    target_corr = data.corr()['target'].drop('target').abs()\n",
    "    sorted_corr = target_corr.sort_values(ascending=False)\n",
    "    \n",
    "    if top_n:\n",
    "        # Return top N features by correlation\n",
    "        return sorted_corr.index[:top_n].tolist()\n",
    "    else:\n",
    "        # Select features with correlation above threshold\n",
    "        selected = sorted_corr[sorted_corr > threshold].index.tolist()\n",
    "        \n",
    "        # Remove highly correlated features\n",
    "        feature_corr = data[selected].corr().abs()\n",
    "        features_to_drop = set()\n",
    "        \n",
    "        for i in range(len(selected)):\n",
    "            if selected[i] in features_to_drop:\n",
    "                continue\n",
    "                \n",
    "            for j in range(i+1, len(selected)):\n",
    "                if selected[j] in features_to_drop:\n",
    "                    continue\n",
    "                    \n",
    "                if feature_corr.iloc[i, j] > threshold:\n",
    "                    # Keep the one with higher correlation to target\n",
    "                    if target_corr[selected[i]] < target_corr[selected[j]]:\n",
    "                        features_to_drop.add(selected[i])\n",
    "                    else:\n",
    "                        features_to_drop.add(selected[j])\n",
    "        \n",
    "        return [f for f in selected if f not in features_to_drop]\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "# Parallelized cross-validation function\n",
    "def cross_validate_rf(X, y, n_folds=5, **params):\n",
    "    \"\"\"Perform cross-validation for RandomForest\"\"\"\n",
    "    fold_size = len(X) // n_folds\n",
    "    \n",
    "    def evaluate_fold(i):\n",
    "        # Create validation fold\n",
    "        val_indices = list(range(i * fold_size, (i + 1) * fold_size))\n",
    "        train_indices = [j for j in range(len(X)) if j not in val_indices]\n",
    "        \n",
    "        X_train_fold, y_train_fold = X[train_indices], y[train_indices]\n",
    "        X_val_fold, y_val_fold = X[val_indices], y[val_indices]\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForest(**params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Evaluate\n",
    "        predictions = model.predict(X_val_fold)\n",
    "        acc = accuracy(y_val_fold, predictions)\n",
    "        return acc\n",
    "    \n",
    "    # Run folds in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        accuracies = list(executor.map(evaluate_fold, range(n_folds)))\n",
    "    \n",
    "    return np.mean(accuracies)\n",
    "\n",
    "class ImprovedRandomForest(RandomForest):\n",
    "    def __init__(self, n_trees=200, max_depth=10, min_samples_split=3, n_features=None):\n",
    "        super().__init__(n_trees, max_depth, min_samples_split, n_features)\n",
    "        self.oob_score_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit with out-of-bag error estimation\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.trees = []\n",
    "        \n",
    "        # For OOB score calculation\n",
    "        oob_predictions = np.zeros((n_samples, len(np.unique(y))))\n",
    "        n_predictions = np.zeros(n_samples)\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            # Bootstrap sample\n",
    "            sample_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            oob_indices = np.array([i for i in range(n_samples) if i not in sample_indices])\n",
    "            \n",
    "            # Train tree\n",
    "            tree = DecisionTree(max_depth=self.max_depth,\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    n_features=self.n_features)\n",
    "            tree.fit(X[sample_indices], y[sample_indices])\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # OOB prediction for this tree\n",
    "            if len(oob_indices) > 0:\n",
    "                oob_pred = tree.predict(X[oob_indices])\n",
    "                \n",
    "                # Count predictions\n",
    "                for j, idx in enumerate(oob_indices):\n",
    "                    oob_predictions[idx, oob_pred[j]] += 1\n",
    "                    n_predictions[idx] += 1\n",
    "        \n",
    "        # Calculate OOB score\n",
    "        oob_score = 0\n",
    "        n_valid = 0\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            if n_predictions[i] > 0:\n",
    "                pred_class = np.argmax(oob_predictions[i])\n",
    "                if pred_class == y[i]:\n",
    "                    oob_score += 1\n",
    "                n_valid += 1\n",
    "        \n",
    "        self.oob_score_ = oob_score / n_valid if n_valid > 0 else 0\n",
    "        return self\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('data/GenreClassData_30s.txt', sep='\\t')\n",
    "data[\"TrackID\"] = range(len(data))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data[data['Type'] == 'Train']\n",
    "test = data[data['Type'] == 'Test']\n",
    "\n",
    "# All features list\n",
    "all_features = [\n",
    "    'zero_cross_rate_mean','zero_cross_rate_std','rmse_mean','rmse_var',\n",
    "    'spectral_centroid_mean','spectral_centroid_var','spectral_bandwidth_mean','spectral_bandwidth_var',\n",
    "    'spectral_rolloff_mean','spectral_rolloff_var','spectral_contrast_mean','spectral_contrast_var',\n",
    "    'spectral_flatness_mean','spectral_flatness_var',\n",
    "    'chroma_stft_1_mean','chroma_stft_2_mean','chroma_stft_3_mean','chroma_stft_4_mean',\n",
    "    'chroma_stft_5_mean','chroma_stft_6_mean','chroma_stft_7_mean','chroma_stft_8_mean',\n",
    "    'chroma_stft_9_mean','chroma_stft_10_mean','chroma_stft_11_mean','chroma_stft_12_mean',\n",
    "    'chroma_stft_1_std','chroma_stft_2_std','chroma_stft_3_std','chroma_stft_4_std',\n",
    "    'chroma_stft_5_std','chroma_stft_6_std','chroma_stft_7_std','chroma_stft_8_std',\n",
    "    'chroma_stft_9_std','chroma_stft_10_std','chroma_stft_11_std','chroma_stft_12_std',\n",
    "    'tempo',\n",
    "    'mfcc_1_mean','mfcc_2_mean','mfcc_3_mean','mfcc_4_mean','mfcc_5_mean','mfcc_6_mean',\n",
    "    'mfcc_7_mean','mfcc_8_mean','mfcc_9_mean','mfcc_10_mean','mfcc_11_mean','mfcc_12_mean',\n",
    "    'mfcc_1_std','mfcc_2_std','mfcc_3_std','mfcc_4_std','mfcc_5_std','mfcc_6_std',\n",
    "    'mfcc_7_std','mfcc_8_std','mfcc_9_std','mfcc_10_std','mfcc_11_std','mfcc_12_std'\n",
    "]\n",
    "\n",
    "features = all_features\n",
    "targets = ['GenreID'] \n",
    "\n",
    "# Prepare data for training\n",
    "train = train.copy()\n",
    "test = test.copy()\n",
    "\n",
    "X_train, y_train = train[features], train[targets]\n",
    "X_test, y_test = test[features], test[targets]\n",
    "\n",
    "X_train_np = X_train.to_numpy()      \n",
    "y_train_np = y_train.to_numpy().ravel()  # flatten from (n,1) to (n,)\n",
    "\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy().ravel()\n",
    "\n",
    "# Get genre names for better visualization\n",
    "genre_names = data['Genre'].unique()\n",
    "genre_id_to_name = dict(zip(data['GenreID'].unique(), genre_names))\n",
    "\n",
    "# Train initial model\n",
    "print(\"\\nTraining initial model...\")\n",
    "clf = RandomForest()\n",
    "clf.fit(X_train_np, y_train_np)\n",
    "\n",
    "predictions = clf.predict(X_test_np)\n",
    "acc = accuracy(y_test_np, predictions)\n",
    "print(f\"Initial accuracy: {acc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = clf.feature_importance(features)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(importance.head(10))\n",
    "\n",
    "# Feature selection\n",
    "print(\"\\nPerforming feature selection...\")\n",
    "selected_features = select_features(X_train_np, y_train_np, features, threshold=0.3, top_n=20)\n",
    "print(f\"Selected {len(selected_features)} features out of {len(features)}\")\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Extract selected features\n",
    "X_train_selected = X_train[selected_features].to_numpy()\n",
    "X_test_selected = X_test[selected_features].to_numpy()\n",
    "\n",
    "# Train with selected features\n",
    "print(\"\\nTraining with selected features...\")\n",
    "clf_selected = RandomForest(n_trees=100, max_depth=10, min_samples_split=3)\n",
    "clf_selected.fit(X_train_selected, y_train_np)\n",
    "selected_predictions = clf_selected.predict(X_test_selected)\n",
    "selected_acc = accuracy(y_test_np, selected_predictions)\n",
    "print(f\"Accuracy with selected features: {selected_acc:.4f}\")\n",
    "\n",
    "# Evaluate initial model with confusion matrix\n",
    "print(\"\\nEvaluating initial model...\")\n",
    "cm = plot_confusion_matrix(y_test_np, predictions, \n",
    "                        class_names=[genre_id_to_name[i] for i in sorted(genre_id_to_name.keys())])\n",
    "\n",
    "# Evaluate selected features model\n",
    "print(\"\\nEvaluating model with selected features...\")\n",
    "cm_selected = plot_confusion_matrix(y_test_np, selected_predictions, \n",
    "                                class_names=[genre_id_to_name[i] for i in sorted(genre_id_to_name.keys())])\n",
    "\n",
    "# Function to evaluate a single parameter combination\n",
    "def evaluate_params(params_tuple):\n",
    "    n_trees, max_depth, min_samples_split, n_features = params_tuple\n",
    "    params = {\n",
    "        'n_trees': n_trees,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'n_features': n_features\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cv_acc = cross_validate_rf(X_train_selected, y_train_np, n_folds=3, **params)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return params, cv_acc, elapsed\n",
    "\n",
    "# Hyperparameter tuning with parallel processing\n",
    "print(\"\\nPerforming hyperparameter tuning with selected features...\")\n",
    "print(\"Using parallel processing with 10 workers\")\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "# Fix the parameter grid definition\n",
    "param_grid = {\n",
    "    'n_trees': [50, 100],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'n_features': [int(np.sqrt(len(selected_features))), len(selected_features) // 2]\n",
    "}\n",
    "\n",
    "# Create parameter combinations\n",
    "param_combinations = []\n",
    "for n_trees in param_grid['n_trees']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            for n_features in param_grid['n_features']:\n",
    "                param_combinations.append((n_trees, max_depth, min_samples_split, n_features))\n",
    "\n",
    "print(f\"Testing {len(param_combinations)} parameter combinations with 3-fold CV\")\n",
    "\n",
    "# Run parameter evaluation in parallel\n",
    "results = []\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_params = {executor.submit(evaluate_params, params): params for params in param_combinations}\n",
    "    \n",
    "    for i, future in enumerate(concurrent.futures.as_completed(future_to_params)):\n",
    "        params_tuple = future_to_params[future]\n",
    "        try:\n",
    "            params, cv_acc, elapsed = future.result()\n",
    "            results.append((params, cv_acc))\n",
    "            print(f\"Combination {i+1}/{len(param_combinations)}: {params}, CV Accuracy: {cv_acc:.4f}, Time: {elapsed:.2f}s\")\n",
    "            \n",
    "            if cv_acc > best_acc:\n",
    "                best_acc = cv_acc\n",
    "                best_params = params.copy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {params_tuple}: {e}\")\n",
    "\n",
    "print(f\"\\nBest hyperparameters with selected features: {best_params}\")\n",
    "print(f\"Best CV accuracy with selected features: {best_acc:.4f}\")\n",
    "\n",
    "# Train improved model with best hyperparameters\n",
    "print(\"\\nTraining improved model with best hyperparameters...\")\n",
    "improved_clf = ImprovedRandomForest(**best_params)\n",
    "improved_clf.fit(X_train_selected, y_train_np)\n",
    "print(f\"Out-of-bag accuracy estimate: {improved_clf.oob_score_:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "improved_predictions = improved_clf.predict(X_test_selected)\n",
    "improved_acc = accuracy(y_test_np, improved_predictions)\n",
    "print(f\"Final test accuracy with improved model: {improved_acc:.4f}\")\n",
    "\n",
    "# Evaluate improved model\n",
    "print(\"\\nEvaluating improved model...\")\n",
    "cm_improved = plot_confusion_matrix(y_test_np, improved_predictions, \n",
    "                                class_names=[genre_id_to_name[i] for i in sorted(genre_id_to_name.keys())])\n",
    "\n",
    "# Feature importance of the final model\n",
    "importance = improved_clf.feature_importance(selected_features)\n",
    "print(\"\\nFeature importance in the final model:\")\n",
    "print(importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
